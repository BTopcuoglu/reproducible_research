**Autotutorial 4: Making a computational notebook collaborative**

***Rationale:*** By definition, science is an iterative and collaborative search
to explain the world around us. Although making an analysis reproducible and
automated are great, that is only part of the mission. By making the raw data
and code publicly accessible, it increases collaboration within and between
research groups. This also helps to insure that not only is an analysis
reproducible, but that it is also done well.

***Incoming:***

* Knowledge:

  * Familiarity with the scientific method as an iterative process
  * Knowledge of different types of public databases
  * Experience requesting strains and reagents from others for wetlab-based
    experiments

* Misconceptions: The end goal of a project is a paper, rather than building to
  a series of papers

***Conceptually difficult material:***

* Releasing data to the public
* Making analysis accessible to the research supervisor or those that do not
  understand code

***Learning goals:***

* *Primary:* Science is an iterative process, were reproducibility is only the
  starting point to moving the field forward
* *Secondary:* Understand how to use technology to engage in peer review of
  computational analysis
* *Technology:* GitHub is used throughout this autotutorial because it's free,
  popular, has a lot of features, and for academic users their repositories can
  be made private or public

***Learning outcomes:***

* Appreciate the value of different platforms for archiving raw data
* Experience forking, modifying, and submitting/receiving pull requests
* Articulate the strengths and weaknesses of different open source licenses

***Activities (~4 hrs):***

* **Case study 1:** A research group has their paper accepted at
  NatureScienceCell and another researcher that is perceived as being a
  competitor reaches out to them to get access to their raw data and data
  analysis pipeline. What would you suggest? How would you argue? This case
  study emphasizes the importance of science as an iterative and collaborative
  affair. (10 min)
* **Presentation:** Discussion of open science concepts including licensing and
  open vs. closed source code. (20 min)
* **Reflection:** Does your laboratory have a consistent practice on whether to
  make repositories public or private during development? What license does
  your lab prefer? Why? (15 min)
* **Go explore:** Journals vary in their requirements for making data accessible
  and methods transparent. Go to the websites for your five favorite journals
  and describe their policies. Which of the journals seems the most "open"? (20
  min)
* **Case study 2:** Congratulations, you just published a paper with a
  reproducible analysis! What hurdles remain to being fully reproducible? This
  case study will get the participants to realize that they now have to make
  their data and source code accessible and that there are limitations and
  benefits to the various approaches. (10 min)
* **Presentation:** Overview of storage possibilities including personal lab
  servers, public databases, and commercial options (e.g. FigShare,
  Amazon, GitHub) (15 min)
* **Go explore:** Look at your five favorite papers. How accessible are the
  original raw data? Are there metadata about the data? Is the analysis
  pipeline available? (15 min)
* **Presentation:** Submitting data to a commercial repository (15 min)
* **Activity:** Submit a data file to DataDryad and automate the retrieval of
  that file (15 min)
* **Demonstration:** Creating a presence on GitHub; pull/push workflow (20 min)
* **Activity:** Post repository to GitHub (30 min)
* **Demonstration:** Forking, pull requests, and code review (20 min)
* **Activity:** Collaborative coding, code review (30 min)
* **Extend:** Post your repository to GitHub, pick a license, and privacy
  setting.
* **Extend:** engage your PI or lab mates in conducting code review of each
  other's code

***Assessment:***

* Time spent on case studies and presentations
* Go explore results
* How well results match what we are looking for in the canned activities
* Demonstration that they got data in and out of DataDryad
* Have they setup their public repository?
* How many repositories in their account after 30, 50, or 100 days?
* Number and frequency of forks and pull requests in the "Extend" activities
* Indication from future publications that automated analyses are being
  published
